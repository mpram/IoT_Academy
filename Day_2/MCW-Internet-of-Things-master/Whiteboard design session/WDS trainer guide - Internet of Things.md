![Microsoft Cloud Workshops](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/master/Media/ms-cloud-workshop.png 'Microsoft Cloud Workshops')

<div class="MCWHeader1">
Internet of Things
</div>

<div class="MCWHeader2">
Whiteboard design session trainer guide
</div>

<div class="MCWHeader3">
June 2020
</div>

Information in this document, including URL and other Internet Web site references, is subject to change without notice. Unless otherwise noted, the example companies, organizations, products, domain names, e-mail addresses, logos, people, places, and events depicted herein are fictitious, and no association with any real company, organization, product, domain name, e-mail address, logo, person, place or event is intended or should be inferred. Complying with all applicable copyright laws is the responsibility of the user. Without limiting the rights under copyright, no part of this document may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording, or otherwise), or for any purpose, without the express written permission of Microsoft Corporation.

Microsoft may have patents, patent applications, trademarks, copyrights, or other intellectual property rights covering subject matter in this document. Except as expressly provided in any written license agreement from Microsoft, the furnishing of this document does not give you any license to these patents, trademarks, copyrights, or other intellectual property.

The names of manufacturers, products, or URLs are provided for informational purposes only and Microsoft makes no representations and warranties, either expressed, implied, or statutory, regarding these manufacturers or the use of the products with any Microsoft technologies. The inclusion of a manufacturer or product does not imply endorsement of Microsoft of the manufacturer or product. Links may be provided to third party sites. Such sites are not under the control of Microsoft and Microsoft is not responsible for the contents of any linked site or any link contained in a linked site, or any changes or updates to such sites. Microsoft is not responsible for webcasting or any other form of transmission received from any linked site. Microsoft is providing these links to you only as a convenience, and the inclusion of any link does not imply endorsement of Microsoft of the site or the products contained therein.

Â© 2020 Microsoft Corporation. All rights reserved.

Microsoft and the trademarks listed at <https://www.microsoft.com/legal/intellectualproperty/Trademarks/Usage/General.aspx> are trademarks of the Microsoft group of companies. All other trademarks are property of their respective owners.

**Contents**

- [Trainer information](#trainer-information)
  - [Role of the trainer](#role-of-the-trainer)
  - [Whiteboard design session flow](#whiteboard-design-session-flow)
  - [Before the whiteboard design session: How to prepare](#before-the-whiteboard-design-session-how-to-prepare)
  - [During the whiteboard design session: Tips for an effective whiteboard design session](#during-the-whiteboard-design-session-tips-for-an-effective-whiteboard-design-session)
- [Internet of Things whiteboard design session student guide](#internet-of-things-whiteboard-design-session-student-guide)
  - [Abstract and learning objectives](#abstract-and-learning-objectives)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study)
    - [Customer situation](#customer-situation)
    - [Customer needs](#customer-needs)
    - [Customer objections](#customer-objections)
    - [Infographic of common scenarios](#infographic-of-common-scenarios)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution)
  - [Step 3: Present the solution](#step-3-present-the-solution)
  - [Wrap-up](#wrap-up)
  - [Additional references](#additional-references)
- [Internet of Things whiteboard design session trainer guide](#internet-of-things-whiteboard-design-session-trainer-guide)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study-1)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution-1)
  - [Step 3: Present the solution](#step-3-present-the-solution-1)
  - [Wrap-up](#wrap-up-1)
  - [Preferred target audience](#preferred-target-audience)
  - [Preferred solution](#preferred-solution)
  - [Checklist of preferred objection handling](#checklist-of-preferred-objection-handling)
  - [Customer quote (to be read back to the attendees at the end)](#customer-quote-to-be-read-back-to-the-attendees-at-the-end)

# Trainer information

Thank you for taking time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.

- Stimulates the participant's thinking.

- Involves the participant in the learning process.

- Manages the learning process (on time, on topic, and adjusting to benefit participants).

- Ensures individual participant accountability.

- Ties it all together for the participant.

- Provides insight and experience to the learning process.

- Effectively leads the whiteboard design session discussion.

- Monitors quality and appropriateness of participant deliverables.

- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs and technical requirements

- Current customer infrastructure and architecture

- Potential issues, objectives and blockers

**Step 2: Design a proof of concept solution (60 minutes)**

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.

- Determine customer's business needs to address your solution.

- Design and diagram your solution.

- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

**Outcome**

Present solution to your customer:

- Present solution

- Respond to customer objections

- Receive feedback

**Wrap-up (15 minutes)**

- Review preferred solution

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.

- Become familiar with all key points and activities.

- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.

- Prior to the whiteboard design session, discuss the case study to pick up more ideas.

- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.

- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.

- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

**\*Have fun**! Encourage participants to have fun and share!\*

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience, and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a little silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Internet of Things whiteboard design session student guide

## Abstract and learning objectives

In this whiteboard design session, you will work with a group to design an implementation of an end-to-end IoT solution simulating high velocity data emitted from smart meters and analyzed in Azure. You will design a lambda architecture, filtering a subset of the telemetry data for real-time visualization on the hot path, and storing all the data in long-term storage for the cold path.

At the end of this whiteboard design session, you will be better able to design an IoT solution implementing device registration with the IoT Hub Device Provisioning Service and visualizing hot data with Power BI.

## Step 1: Review the customer case study

**Outcome**

Analyze your customer's needs.

Timeframe: 15 minutes

Directions: With all participants in the session, the facilitator/SME presents an overview of the customer case study along with technical tips.

1. Meet your table participants and trainer.

2. Read all of the directions for steps 1-3 in the student guide.

3. As a table team, review the following customer case study.

### Customer situation

Fabrikam provides services and smart meters for enterprise energy (electrical power) management. Their **You-Left-The-Light-On** service enables the enterprise to understand their energy consumption, not just as monthly totals, but also with a more detailed breakdown providing kilowatt-hours consumed by day. These reports are used by the enterprise to identify operational patterns and address spikes that could be mitigated with better procedures.

Today, Fabrikam provides this information as printed reports that are mailed to customer enterprises monthly. However, regulations are changing and are encouraging Fabrikam to upgrade their offering and increase their market share.

The power company, City Power & Light (CPL), has an offering for enterprises that participate in their Smart Energy program. The program provides significantly discounted rates on electricity for enterprises that are managing their electricity consumption, with awards and rebates when customers demonstrate energy savings across various areas of the enterprise including reducing consumption during peak utility hours and improving efficiencies of energy used.

The requirements for meeting the criteria of the Smart Energy program center on the collection and analysis of detailed energy consumption data. In short, telemetry data must be collected in at least 30-minute intervals; the data needs to include a timestamp, the identifier of the device must be metered, and the energy consumed over the reporting interval. These devices report their telemetry to one of a few energy management solution providers authorized by City Power & Light. These providers store, process, and tag the telemetry data on behalf of City Power & Light and provide reporting Application Programming Interfaces (APIs) that CPL can use to calculate awards and rebates.

Fabrikam would like to become an authorized energy management solution provider. Today, they collect all telemetry data within on-premises servers and all telemetry data for the enterprises they monitor is stored as flat files on their Storage Area Network (SAN). They only keep the current quarter's worth of telemetry data loaded in their on-premises operational data store (running SQL Server 2012).

According to their Director of Analytics, Sam George, "We are investigating a move to the cloud to help our customers not only to meet CPL's data collection and reporting requirements, but also become the number one energy management solution provider." They are intending to enable their enterprise customers with a web-based dashboard where they can see historical trends of power consumption, no longer limited to the current quarter.

Fabrikam believes that small optimizations made using intra-day data is the best way to help its enterprise customers receive awards from CPL and cut costs from electricity use in the end. Within the customer dashboard, it would like to provide customized alerts for when a customer's electrical demand is "out of the ordinary" for that customer at that point of the day and could cause them to miss CPL rebates and energy savings. In short, they would like to provide their customers with a "hot" dashboard of near-real-time metrics and ultimately offer predictions over the current 15-minute period of data (albeit the predictive analytics is out of scope for this first effort) and support for the analysis of "cold" historical data (such as month over month, and year over year comparisons).

To accomplish this, Fabrikam will aggressively adjust its reporting interval for its smart meters to one-minute intervals for all of its 20,000 business customers and 200K smart meters. Each beacon generated by a meter is about 100 bytes in size and is sent to Fabrikam services via an HTTP POST over Transport Layer Security (TLS), Message Queuing Telemetry Transport (MQTT), or Advanced Message Queueing Protocol (AMQP).

In addition to collecting telemetry, Fabrikam not only seeks to gain competitive advantage by providing centralized reporting on their customers' smart meter data but would also like their new backend to be able to authorize which devices can send telemetry. They want a solution that will in the future enable Fabrikam's device administration website to send control messages to a particular device.

### Customer needs

1. We expect to have a "hot" and "cold" path of data.

2. The "hot" path will select certain data streaming in from the devices that we need to process in real time to drive updates to the customer dashboard.

3. While the "hot" path may focus on a subset of the incoming data, the "cold" path will store and process everything.

4. Because the "hot" path provides the data for the current day's operations, we only need to process the "cold" path on a nightly basis. Our customers are happy with performing their analysis starting with the last full day of data, which is inherently yesterday.

5. We want a way to visualize our data flow, the steps taken on the data, and the status of our data flow on a single screen.

6. We want to understand the ways in which we can scale the solution to accommodate future growth in terms of number of customers, meters, and the size of the data.

### Customer objections

1. We are considering an out-of-the-box time series database solution. Are there options for this on Azure?

2. We have a mix of large enterprise customers and many small-to-medium business (SMB) customers, which adds up to a lot of telemetry data to ingest, can Azure really handle it?

3. Can Azure handle a lambda architecture?

4. We have heard of Azure IoT Solution Accelerators, do these offer a good starting point for us?

5. Some of our customers require their IoT devices to communicate in a firewall-friendly way without opening up additional incoming or outgoing ports. What options do we have to accomplish this?

### Infographic of common scenarios

![A sample Internet of Things workflow is displayed, which is broken into On-Premises and Azure services. On-premises shows a building that has multiple devices communicating to the cloud. The Azure services ingest the telemetry through Event Hubs or IoT Hubs. From there Stream Processing functions through HDInsight Storm, HDInsight Spark, and Stream Analytics take place. Batch storage holds processed data in Data Lake and Storage Blobs services. Batch Processing takes place using HDInsight, Batch, SQL Data Warehouse and Machine Learning services. Views of this processed data are made available through HDInsight HBase, SQL Data Warehouse, and Search Services. Data is consumed by Analytics clients such as Power BI, Web Apps, and API applications. Stream near real time data is also made available to analytics clients through the use of Redis Cache, Cosmos DB, and SQL Databases - these services are fed directly from the stream processing engine(s).](./media/common-scenarios.png 'Common Internet of Things scenarios')

## Step 2: Design a proof of concept solution

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 60 minutes

**Business needs**

Directions: With all participants at your table, answer the following questions and list the answers on a flip chart:

1. Who should you present this solution to? Who is your target customer audience? Who are the decision makers?

2. What customer business needs do you need to address with your solution?

**Design**

Directions: With all participants at your table, respond to the following questions on a flip chart:

_High-level architecture_

1. Without getting into the details (the following sections will address the particular details), diagram your initial vision for handling the top-level requirements for data ingest, hot and cold path processing, storage of telemetry data, and reporting. You will refine this diagram as you proceed.

_Device to cloud communication_

1. What is the anticipated volume in messages per second and in megabytes (MB) per second that Fabrikam will need to support given their customer base?

2. How would you propose they ingest that quantity of messages? What Azure service would you recommend and why? At what initial scale?

3. Diagram the device to cloud communication.

   - What protocol would they use in sending telemetry from the smart meter devices to the service used for message ingest?

   - What is the format of the message sent to the ingest endpoint?

   - What service endpoints do the devices talk to?

_Device provisioning_

1. Keeping the Azure service, you selected for ingest of telemetry data from the smart meters in mind, diagram how Fabrikam should handle the following three flows related to the provisioning of new smart meters at a customer site:

   - Create device identity

   - Install device

   - Activate device

_"Hot" path processing_

The "hot" path for Fabrikam is defined as the processing of the data as it arrives in near real-time fashion. A history of data collected this way need only be maintained for the current 24-hour period, providing a moving average of measures for each device across a 5-minute period. It needs to be made available as quickly as possible for consumption to be available on the web-based reporting interface.

1. How would you select out the "hot" data? Choosing between the stream processing options Azure Stream Analytics and Storm on HDInsight, which would you recommend for this scenario and why?

2. Explain how you could build the solution using Azure Stream Analytics:

   - What type of window would you use? What does your query look like?

   - How many Streaming Units would you need? Explain how you calculated it.

3. Explain how you could build the solution using Storm on HDInsight:

   - What are the high-level steps you would need to take?

4. How would you store the "hot" data for consumption by the web dashboard?

   - Estimate the write throughput you would require. Does your selected store support it?

_"Cold" path processing_

The "cold" path for Fabrikam is highlighted by the notion that Fabrikam does not want to lose any telemetry data from its devices, so it can be prepared to ask new questions of the data over time in the future. The second requirement is to support the calculation of period over period reports for each meter (for example, year over year, month over month, and week over week). For each new full day of data added, it amounts to updating the summary statistics for the current year, current month, and current week with a three-number summary consisting of minimum, mean, and maximum. Fabrikam would prefer the implementation of these reports to be done using syntax similar to SQL. Assume you want to create the summary statistics for each device by year.

1. How would you structure the output of blobs from your stream processing component? Draw an example hierarchy.

2. What would you use to query these blob files?

3. How would you orchestrate the processing and retain visibility into the status of the data flow? How would you configure this data flow? Be specific on what activities you would use.

_Cloud to device communication_

1. Diagram how commands from the Fabrikam device management website would flow to the target device. Be specific and identify endpoints used and protocols selected.

**Prepare**

Directions: With all participants at your table:

1. Identify any customer needs that are not addressed with the proposed solution.

2. Identify the benefits of your solution.

3. Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

**Outcome**

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 30 minutes

**Presentation**

Directions:

1. Pair with another table.

2. One table is the Microsoft team and the other table is the customer.

3. The Microsoft team presents their proposed solution to the customer.

4. The customer makes one of the objections from the list of objections.

5. The Microsoft team responds to the objection.

6. The customer team gives feedback to the Microsoft team.

7. Tables switch roles and repeat Steps 2-6.

## Wrap-up

Timeframe: 15 minutes

Directions: Tables reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Additional references

|                         |                                                                                       |
| ----------------------- | ------------------------------------------------------------------------------------- |
| **Description**         | **Links**                                                                             |
| IoT Hubs                | <https://azure.microsoft.com/documentation/articles/iot-hub-devguide/>                |
| IoT Hub Message Routing | <https://docs.microsoft.com/azure/iot-hub/tutorial-routing>                           |
| Event Hubs              | <https://azure.microsoft.com/documentation/articles/event-hubs-overview/>             |
| Stream Analytics        | <https://azure.microsoft.com/documentation/articles/stream-analytics-introduction/>   |
| Data Factory            | <https://azure.microsoft.com/documentation/articles/data-factory-introduction/>       |
| Storm                   | <https://azure.microsoft.com/documentation/articles/hdinsight-storm-overview/>        |
| Hive                    | <https://azure.microsoft.com/documentation/articles/hdinsight-use-hive/>              |
| Spark                   | <https://azure.microsoft.com/documentation/articles/hdinsight-apache-spark-overview/> |
| Azure Databricks        | <https://azure.microsoft.com/services/databricks/>                                    |

# Internet of Things whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your table participants to introduce yourself as the trainer.

- Ask, "What questions do you have about the customer case study?"

- Briefly review the steps and timeframes of the whiteboard design session.

- Ready, set, go! Let the table participants begin.

## Step 2: Design a proof of concept solution

- Check in with your tables to ensure that they are transitioning from step to step on time.

- Provide some feedback on their responses to the business needs and design.

  - Try asking questions first that will lead the participants to discover the answers on their own.

- Provide feedback for their responses to the customer's objections.

  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which table will be paired with your table before Step 3 begins.

- For the first round, assign one table as the presenting team and the other table as the customer.

- Have the presenting team present their solution to the customer team.

  - Have the customer team provide one objection for the presenting team to respond to.

  - The presentation, objections, and feedback should take no longer than 15 minutes.

  - If needed, the trainer may also provide feedback.

## Wrap-up

- Have the table participants reconvene with the larger session group to hear the facilitator/SME share the following preferred solution.

## Preferred target audience

- Sam George, Director of Analytics for Fabrikam

- The primary audience is the business and technology decision makers. From the case study scenario, this would include the Director of Analytics. Usually we talk to the infrastructure managers who report to the chief information officers (CIOs), or to application sponsors (like a vice president [VP] line of business [LOB], or chief marketing officer [CMO]), or to those that represent the business unit IT or developers that report to application sponsors.

## Preferred solution

_High-level architecture_

1. Without getting into the details (the following sections will address the details), diagram your initial vision for handling the top-level requirements for data ingest, hot and cold path processing, storage of telemetry data, and reporting. You will refine this diagram as you proceed.

   After speaking with its supportive team at Microsoft, Fabrikam decided that Azure would in fact be the right choice for their platform. They decided on a solution that at a high level appears as follows:

   ![Diagram of preferred solution, displaying smart meter telemetry being ingested into IoT Hub, then processed via Stream Analytics into hot and cold paths. More about the diagram is described in the text following this diagram.](./media/preferred-solution-architecture.png 'Preferred solution architecture')

   Messages are ingested from the Smart Meters via IoT Hub and temporarily stored there. A Stream Analytics job pulls telemetry messages from IoT Hub and sends the messages to two different destinations. There are two Stream Analytics jobs, one that retrieves all messages and sends them to Blob Storage (the cold path), and another that selects out only the important events needed for reporting in real time (the hot path) from the website hosted in Azure Web Apps. Data entering the hot path will be reported on using Power BI visualizations and reports. For the cold path, Azure Databricks can be used to apply the batch computation needed for the reports at scale. The entire cold-path processing pipeline could be coordinated with Azure Data Factory.

   Other alternatives for processing of the ingested telemetry would be to use an HDInsight Storm cluster, a WebJob or Azure function running the EventProcessorHost in place of Stream Analytics, or Azure Databricks running with Spark streaming. Depending on the type of message filtering being conducted for hot and cold stream separation, IoT Hub Message Routing might also be used. A couple things to consider are that adding additional routing endpoints could add some minor end-to-end latency to device-to-cloud telemetry messages (usually less than 500ms), and that you are restricted to outputting in either Apache Avro or JSON (in preview) formats when writing to Blob storage. An important limitation to keep in mind for Stream Analytics is that it is very restrictive on the format of the input data it can process: the payload must be UTF8 encoded JSON, UTF8 encoded CSV (fields delimited by commas, spaces, tabs, or vertical pipes), or AVRO, and it must be well formed. If any devices transmitting telemetry cannot generate output in these formats (e.g., because they are legacy devices), or their output can be not well formed at times, then alternatives that can better deal with these situations should be investigated. Additionally, any custom code or logic cannot be embedded with Stream Analytics---if greater extensibility is required, the alternatives should be considered.

   > **Note**: The preferred solution is only one of many possible, viable approaches.

_Device to cloud communication_

1. What is the anticipated volume in messages per second and in megabytes (MB) per second that Fabrikam will need to support given their customer base?

   They would need to support 3,333 messages per second (200,000 meters x 1 msg per minute / 60 s per minute = 3,333.33 messages per second) and about 0.32 MB per second (100 bytes per message x 3,333.33 messages per second = 333,333 bytes per second / (1024 KB/B x 1024 MB/KB) \~= 0.32 MB per second).

2. How would you propose they ingest that quantity of messages? What Azure Service would you recommend and why? At what initial scale?

   While they could use Event Hubs, Fabrikam should consider IoT Hubs because on top of the high-performance message ingest from the device to cloud, it provides the functionality for cloud to device communication, a device registry, and the ability to correlate incoming messages with the device that sent them. To handle the anticipated 288M messages per day, they would need a single IoT Hub Unit at the S3 scale.

3. Diagram the device to cloud communication.

   - What protocol would they use in sending telemetry from the smart meter devices to the service used for message ingest?

     HTTPS POST, AMQP (Advanced Message Queueing Protocol), AMQP over WebSockets, MQTT, or MQTT over WebSockets.

   - What is the format of the message sent to the ingest endpoint?

     Binary, typically a JavaScript Object Notation (JSON), UTF8 encoded string or pure JSON when using HTTPS endpoint.

   - What service endpoints do the devices talk to?

     To an endpoint of the form `http(s)://{IoTHubHostName}.azure-devices.net/devices/{deviceId}/events`

   ![the Device to cloud communication diagram starts with Smart meters. Messages are sent to an IoT Hub via: HTTPS, POST, AMQP, AMQP over WebSockets, MQTT, and MQTT over WebSockets.](./media/device-to-cloud-communication.png 'Device to cloud communication diagram')

_Device provisioning_

1. Keeping the Azure service you selected for ingest of telemetry data from the smart meters in mind, diagram how Fabrikam should handle the following three flows related to the provisioning of new smart meters at a customer site:

   - Create device identity

   - Install device

   - Activate device

   ![IoT device provisioning flow. Devices are registered with IoT Hub identity registry, then they are installed and connected to the network. Activation of devices enables the flow of telemetry data to begin. The IoT Device Provisioning Flow is broken into three steps: Create Device Identity, Install Device, and Activate Device. The Create Device Identity workflow begins with an Admin user who sets the pre-shared key on the Smart Meter Device, and registers the device identity at the Fabrikam Device Admin Website. The Device is registered in the IoT Hub identity registry, but the deviceStatus is set to disabled. The Install Device workflow has the device powered up and connected to the network. The Activate Device workflow has the Device Admin activating the device on the Fabrikam Device Admin Website. Using HTTP PUT, and IoT Hub, activation enables set deviceStatus to enabled, to allow connection. Device metadata is updated to reflect the customer who owns it, and the installation location.](./media/iot-device-provisioning-flow.png 'IoT Device Provisioning Flow')

_"Hot" path processing_

1. How would you select out the "hot" data? Choosing between the stream processing options Azure Stream Analytics and Storm on HDInsight, which would recommend for this scenario and why?

   In this scenario, Azure Stream Analytics and Storm can both perform the necessary window computations and write the output to SQL Database. A primary consideration is the effort required. In the case of Stream Analytics, Fabrikam need only author a query using the Stream Analytics Query Language, but for Storm, Fabrikam would need to author, package, and deploy a Java application or if using HDInsight with Storm on a Windows Cluster, they would use SCP.NET to write Storm topologies using .NET.

2. Explain how you could build the solution using Azure Stream Analytics:

   - What type of window would you use? What does your query look like?

   Use tumbling window with 5-minute duration. The query would look similar to the following:

   ```sql
   SELECT AVG(EnergyUsed) AS Average, DeviceId
   FROM Input1
   GROUP BY TumblingWindow(minute, 5), DeviceId
   ```

   - How many streaming units would you need? Explain how you calculated it.

     Since the expected volume of data flowing thru the Event Hub is \~325 kilobytes (KB)/s only one Streaming Unit (which supports up to 1 MB/s) is required.

3. Explain how you could build the solution using Storm on HDInsight.

   - What are the high-level steps you would need to take?

     - Provision an HDInsight cluster.

     - Author a topology project in Java or C# (when using SCP.NET) that uses the EventHubSpout.

     - Compile and package the project.

     - Manually submit the project using your cluster's Storm Dashboard.

     - Alternately, if developing in Visual Studio using the HDInsight Tools for Visual Studio, right-click the project and choose **Submit to Storm on HDInsight**.

   > **Note**: In step 2, support for tumbling windows is something that needs to be built upon the primitives provided by Storm. There are open source projects that can help by providing these higher-level event processing functions (such as FlowMix, [[https://github.com/calrissian/flowmix]](https://github.com/calrissian/flowmix)), but it is important to recognize that this functionality is not a part of the baseline Storm.

4. How would you store the "hot" data for consumption by the web dashboard? Estimate the write throughput you would require, does your selected store support it?

   If Stream Analytics was used, then you could store the hot data in SQL DB. Using Storm, your options broaden to SQL DB or HBase, but SQL DB would still be the preferred option because it would require more development for the Web Dashboard to "join" additional data to the telemetry that is used in the reports (such as friendly device names, labels, and so on).

   When averaged to a 5-minute window per device, the write requirement becomes very low \~0.06 MB/s. SQL DB S0 or higher could easily handle this load.

_"Cold" path processing_

1. How would you structure the output of blobs from your stream-processing component? Draw an example hierarchy.

   One approach is to use a hierarchy with the year at the root, followed month, day, and hour. This structure is interpreted when declaring an external table (so the files within the folder do not need to also contain the year, month, day, and hour data).

   - year = 2015
   - month = 12
   - day = 05
   - hour = 09

2. What would you use to query these blob files?

   A Databricks notebook should be used to query the blob files. The storage account containing the blob files can be mounted in Databricks File System (DBFS), or they could be accessed via a **wasbs** path in Databricks. If HDInsight is used, HiveQL or Spark SQL can be used for querying the files. Azure SQL Data Warehouse can also be used to read from Azure Storage blobs.

3. How would you orchestrate the processing and retain visibility into the status of the data flow? How would you configure this data flow? Be specific on what activities you would use.

   You can orchestrate the processing and get a status in flowchart form of the data flow by using Azure Data Factory (ADF). In ADF, you would configure a pipeline that has as input a dataset that is pointing to blob storage (where the telemetry data lives), and as output a dataset that writes the data to SQL Database. The pipeline would contain an activity to execute a Databricks notebook, which would start an Azure Databricks cluster on-demand, perform any data processing required, and output the results back into blob storage. A copy activity in the pipeline would then write the data to SQL Database. Alternatively, a single Hive activity that executes a previously defined Hive query (stored in blob storage) could be used. An on-demand HDInsight cluster would be used for computation of the Hive query.

_Cloud to device communication_

1. Diagram how commands from the Fabrikam device management website would flow to the target device. Be specific and identify endpoints used and protocols selected.

   ![The IoT Cloud to Device workflow begins with an Azure cloud, in which are contained Fabrikam's Device Management website, and the IoT Hub. Messages are transferred via MQTT, HTTP Polling, or AMQP.](./media/iot-cloud-to-device-workflow.png 'IoT Cloud to Device workflow')

## Checklist of preferred objection handling

1. We are considering an out-of-the-box time series database solution. Are there options for this on Azure?

   There are two primary options you can select, depending on the level of flexibility you desire. The first option is a hosted Azure service that does not require any software installation or server management, called Time Series Insights. It is built to ingest massive amounts of time series data, specifically IoT devices. You can use it to quickly create filters, build reports, and import from more than one source so you can compare the data in one location.

   Another option for hosting a times series database in Azure is Open TSDB, which leverages HBase, and is supported as a service that is a part of Azure HDInsight.

2. We have a mix of large enterprise customers and many SMB customers, which adds up to a lot of telemetry data to ingest, can Azure really handle it?

   Absolutely. Event Hubs is designed for massive scale of streaming message ingest. As an example, it can handle 1 billion events per day, and that is just at a scale of 12 TU (Throughput Units) which costs ~\$162/month. You can scale to 20 TUs on your own, or more if you call Microsoft to increase your quota.

3. Can Azure handle a lambda architecture?

   Yes. There are a variety of services in Azure that can be used in combination according to the customer requirements to accomplish the batch or streaming ingest, the hot and cold paths of processing (both stream and batch), transactional and write-once-read-many storage and data pipelining. Many services overlap in their capability to meet the needs of the lambda architecture---it is important to take the time to create a project plan that educates the customer with the Azure options in the analytics pipeline and identifies why specific services were recommended for the final solution.

4. We have heard of Azure IoT Solution Accelerators, do these offer a good starting point for us?

   Azure IoT Solution Accelerators offer pre-configured solutions that automate the provisioning and configuration of scenario-oriented solutions that leverage a combination of Azure services. The idea is that a customer can take the solution accelerator deployed by one of the Azure IoT Solution Accelerators and customize it to meet their needs. The Remote Monitoring IoT Solution Accelerator provides a solution that is reasonably close to the needs of Fabrikam (see the illustration below for the architecture deployed by the Remote Monitoring solution), although some additional work is needed to handle the cold path processing, to capitalize on Spark, or to use Azure SQL Data Warehouse.

   ![Screenshot of the Remote Monitoring Azure IoT Solution Accelerator workflow. At a very high level, the solution includes the following: Web App, Cosmos DB (Device Registry), Logic Apps, Azure Active Directory, Microservices on VMs, Azure Stream Analytics, an IoT Hub, and simulated devices.](./media/azure-iot-solution-accelerator.png 'Azure IoT Solution Accelerator')

5. Some of our customers require their IoT devices to communicate in a firewall-friendly way without opening up additional incoming or outgoing ports. What options do we have to accomplish this?

   It is common for companies to install their IoT devices behind a firewall within private, isolated networks. Oftentimes, the firewalls that are installed on the edge of the network are configured to block incoming communications, including the type used to send commands to the devices over the internet. Since one of the solution requirements is to enable cloud-to-device communication, we need a secure, firewall-friendly way to communicate between these isolated networks and the public internet. Azure IoT Hub Device Streams allow us to use the outbound connection that is already established for the devices to communicate with IoT Hub, and enable that connection to also receive inbound connections from the cloud to these devices. In this way, IoT Hub acts as a proxy between the devices and external services that are otherwise blocked from direct communications through the firewall. With Device Streams, only the outbound port 443 is used. You do not need to open any inbound ports on the device or its network. To ensure secure communication between devices and services, or applications, IoT Hub Device Streams enforces authentication by requiring the devices and services communicating with them to authenticate using their IoT Hub credentials. All traffic sent over a device stream is always encrypted using TLS, regardless of whether the application sending communication encrypts its messages. Another benefit to using Device Streams is that the streams are fully compatible with the TCP/IP stack. This makes it easy to integrate into proprietary device applications or off-the-shelf TCP/IP applications such as SSH/RDP, web, file transfer, etc.

   The Device Stream workflow is as follows:

   - The device and external service both authenticate with IoT Hub. IoT Hub then provides both the device and service with an authentication token to a streaming endpoint.
   - The device and service create WebSocket clients using the authentication token. Reliability and ordering of messages on par with TCP. All communications encrypted over WebSocket's TLS channel.
   - The service initially connects to the device by name, not IP.
   - The device can decide whether to accept the connection.
   - The service is notified of the result of device accepting the stream and proceeds to create its own WebSocket client to the streaming endpoint. Similarly, it receives the streaming endpoint URL and authentication information from IoT Hub.
   - This workflow happens transparently, reducing or eliminating additional code on both the device and service side.

   ![Diagram showing communication between devices and services using Device Streams.](media/iot-hub-device-streams.png 'Device Streams')

## Customer quote (to be read back to the attendees at the end)

"With Azure we can support tremendous volumes of data, while not giving up our ability to manage and monitor the processes behind it."

Sam George, Director of Analytics, Fabrikam
